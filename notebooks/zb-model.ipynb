{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Pytorch Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.load('../data/processed/train_dataset.pt')\n",
    "test = torch.load('../data/processed/test_dataset.pt')\n",
    "val = torch.load('../data/processed/val_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_dataloader(val):\n",
    "    return DataLoader(val, shuffle=True, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dl = val_dataloader(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "step, batch = next(enumerate(val_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in enumerate(train):\n",
    "    \n",
    "    input_ids, attn_mask, label_ids = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nick: Hey Dan, hey Eugenio Eugenio: Hi Dan: Hi, Nick Nick: Did you see that weird German guy yesterday at the party? He looked like fucking Harry Potter Dan: Lol! True Eugenio: And you look like fucking Hagrid, Nick XD'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There was an odd German at the party yesterday who resembled Harry Potter. Nick looks like Hagrid.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(label_ids, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in enumerate(train_loader):\n",
    "    \n",
    "    input_ids, attn_mask, label_ids = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': attn_mask,\n",
    "    'labels': label_ids,\n",
    "    'return_dict': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = output['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.5333, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import pytorch_lightning as pl\n",
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "\n",
    "class BartLightningModule(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 pretrained_nlp_model: str,\n",
    "                 train_dataset: str,\n",
    "                 test_dataset: str,\n",
    "                 val_dataset: str,\n",
    "                 batch_size: int,\n",
    "                 learning_rate: float = 3e-05):\n",
    "        \"\"\"\n",
    "        A Pytorch-Lightning Module that trains Bart from the  HuggingFace transformers \n",
    "        library.\n",
    "        \n",
    "        :param pretrained_nlp_model: (str) the name of the pretrained mode you want to use.\n",
    "        :param train_dataset: (str) path to pytorch dataset containing train data.\n",
    "        :param test_dataset: (str) path to pytorch dataset containing test data.\n",
    "        :param val_dataset: (str) path to pytorch dataset containing validation data.\n",
    "        :param batch_size: (int) Number of data points to pass per batch in the train, test, and validation sets.\n",
    "        :param learning_rate: (float) Initial Learning Rate to set.\n",
    "        :returns: None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = int(batch_size)\n",
    "        self.train_dataset = str(train_dataset)\n",
    "        self.test_dataset = str(test_dataset)\n",
    "        self.val_dataset = str(val_dataset)\n",
    "        self.hparams.learning_rate = learning_rate\n",
    "        \n",
    "        self.bart = BartForConditionalGeneration.from_pretrained(pretrained_nlp_model)\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(pretrained_nlp_model)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Run through NLP Model\n",
    "        output = self.bart(**x)\n",
    "\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        input_ids, attn_mask, labels = batch\n",
    "        \n",
    "        x = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attn_mask,\n",
    "            'labels': labels,\n",
    "            'return_dict': True\n",
    "        }\n",
    "        \n",
    "        # Run through NLP Model\n",
    "        out = self.bart(**x)\n",
    "                \n",
    "        loss = out['loss']\n",
    "        print(f'current_epoch: {self.current_epoch};')\n",
    "        print(f'global_step: {self.global_step}')\n",
    "        print(f'train_loss: {loss};')\n",
    "        \n",
    "        result = pl.TrainResult(minimize=loss)\n",
    "        result.log('train_loss', loss, sync_dist=True, reduce_fx=torch.mean)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attn_mask, labels = batch\n",
    "        \n",
    "        x = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attn_mask,\n",
    "            'labels': labels,\n",
    "            'return_dict': True\n",
    "        }\n",
    "        \n",
    "        # Run through NLP Model\n",
    "        out = self.bart(**x)\n",
    "                \n",
    "        loss = out['loss']\n",
    "        \n",
    "        return {\n",
    "            'logits': out['logits'],\n",
    "            'labels': x['labels'],\n",
    "            'loss': loss.reshape(1, -1),\n",
    "            'summary_ids': self.bart.generate(x['input_ids'], num_beams=4, max_length=90, early_stopping=True)\n",
    "        }\n",
    "  \n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \"\"\"\n",
    "        Runs at the end of the validation epoch. Computing Rouge Scores\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        logits = torch.cat([out['logits'] for out in outputs])\n",
    "        labels = torch.cat([out['labels'] for out in outputs])\n",
    "        losses = torch.cat([out['loss'] for out in outputs])\n",
    "        summary_ids = torch.cat([out['summary_ids'] for out in outputs])\n",
    "        \n",
    "        # Generating Rouge Scores\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        predictions = self.tokenizer.batch_decode(\n",
    "            predictions, \n",
    "            skip_special_tokens=True, \n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        predictions = self.tokenizer.batch_decode(\n",
    "            summary_ids\n",
    "        )\n",
    "        \n",
    "        \n",
    "        references = self.tokenizer.batch_decode(\n",
    "            labels, \n",
    "            skip_special_tokens=True, \n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        self.logger.experiment.add_text(\n",
    "            tag='example_summaries', \n",
    "            text_string=f'''\n",
    "            Model Summary: {predictions[0]}\n",
    "            \n",
    "            Target Summary: {references[0]}''', \n",
    "            global_step=self.global_step, \n",
    "        )\n",
    "        \n",
    "        metric = load_metric(\"rouge\")\n",
    "        metric.add_batch(predictions=predictions, references=references)\n",
    "        rouge_score = metric.compute()\n",
    "\n",
    "        rs_1_low = rouge_score['rouge1'].low\n",
    "        rs_1_mid = rouge_score['rouge1'].mid\n",
    "        rs_1_high = rouge_score['rouge1'].high\n",
    "\n",
    "        rs_L_low = rouge_score['rougeL'].low\n",
    "        rs_L_mid = rouge_score['rougeL'].mid\n",
    "        rs_L_high = rouge_score['rougeL'].high\n",
    "        \n",
    "        result = pl.EvalResult()\n",
    "        result.log('val_loss', losses, sync_dist=True, reduce_fx=torch.mean)\n",
    "        print(f'val_loss: {torch.mean(losses)};')\n",
    "        \n",
    "        result.log('learning_rate', self.hparams.learning_rate)\n",
    "        \n",
    "        # Rouge-1 Score (Unigrams)\n",
    "        ## Low\n",
    "        result.log('rs1_low_precision', rs_1_low.precision, sync_dist=True, reduce_fx=torch.mean)\n",
    "        result.log('rs1_low_recall', rs_1_low.recall, sync_dist=True, reduce_fx=torch.mean)\n",
    "        result.log('rs1_low_fmeasure', rs_1_low.fmeasure, sync_dist=True, reduce_fx=torch.mean)\n",
    "        \n",
    "        ## Mid\n",
    "        result.log('rs1_mid_precision', rs_1_mid.precision, sync_dist=True, reduce_fx=torch.mean)\n",
    "        result.log('rs1_mid_recall', rs_1_mid.recall, sync_dist=True, reduce_fx=torch.mean)\n",
    "        result.log('rs1_mid_fmeasure', rs_1_mid.fmeasure, sync_dist=True, reduce_fx=torch.mean)\n",
    "        \n",
    "        ## High\n",
    "        result.log('rs1_high_precision', rs_1_high.precision, sync_dist=True, reduce_fx=torch.mean)\n",
    "        result.log('rs1_high_recall', rs_1_high.recall, sync_dist=True, reduce_fx=torch.mean)\n",
    "        result.log('rs1_high_fmeasure', rs_1_high.fmeasure, sync_dist=True, reduce_fx=torch.mean)\n",
    "        \n",
    "        # Rouge-L Score\n",
    "        ## Low\n",
    "        result.log('rsL_low_precision', rs_L_low.precision, sync_dist=True, reduce_fx=torch.mean)\n",
    "        result.log('rsL_low_recall', rs_L_low.recall, sync_dist=True, reduce_fx=torch.mean)\n",
    "        result.log('rsL_low_fmeasure', rs_L_low.fmeasure, sync_dist=True, reduce_fx=torch.mean)\n",
    "        \n",
    "        ## Mid\n",
    "        result.log('rsL_mid_precision', rs_L_mid.precision, sync_dist=True, reduce_fx=torch.mean)\n",
    "        result.log('rsL_mid_recall', rs_L_mid.recall, sync_dist=True, reduce_fx=torch.mean)\n",
    "        result.log('rsL_mid_fmeasure', rs_L_mid.fmeasure, sync_dist=True, reduce_fx=torch.mean)\n",
    "        \n",
    "        ## High\n",
    "        result.log('rsL_high_precision', rs_L_high.precision, sync_dist=True, reduce_fx=torch.mean)\n",
    "        result.log('rsL_high_recall', rs_L_high.recall, sync_dist=True, reduce_fx=torch.mean)\n",
    "        result.log('rsL_high_fmeasure', rs_L_high.fmeasure, sync_dist=True, reduce_fx=torch.mean)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids, attn_mask, labels = batch\n",
    "        \n",
    "        x = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attn_mask,\n",
    "            'labels': labels,\n",
    "            'return_dict': True\n",
    "        }\n",
    "        \n",
    "        # Run through NLP Model\n",
    "        out = self.bart(**x)\n",
    "                \n",
    "        loss = out['loss']\n",
    "        print(f'test_loss: {loss};')\n",
    "        \n",
    "        result = pl.EvalResult()\n",
    "        result.log('test_loss', loss, sync_dist=True, reduce_fx=torch.mean)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Recreating the same Adam optimizer used in the author's code.\n",
    "        \n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), \n",
    "            lr=self.hparams.learning_rate, \n",
    "            weight_decay=0.01, \n",
    "            betas=(0.9, 0.999), \n",
    "            eps=1e-08\n",
    "        )\n",
    "        return optimizer\n",
    "    \n",
    "    # overriding optimizer_step() so we can implement the custom learning rate warmup\n",
    "    def optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_idx, second_order_closure=None, on_tpu=False, using_native_amp=False, using_lbfgs=False):\n",
    "        \"\"\"\n",
    "        overriding optimizer_step() so we can implement the custom learning rate warmup.\n",
    "        \n",
    "        For parameter information see docs: \n",
    "        \n",
    "        \"\"\"\n",
    "        # warm up lr\n",
    "\n",
    "        if self.trainer.global_step < 500:\n",
    "            lr_scale = min(1.0, float(self.trainer.global_step + 1) / 500.0)\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg['lr'] = lr_scale * self.hparams.learning_rate\n",
    "\n",
    "        # update params\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(torch.load(self.train_dataset), shuffle=True, batch_size=self.batch_size)\n",
    "\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(torch.load(self.val_dataset), shuffle=False, batch_size=self.batch_size)\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(torch.load(self.test_dataset), shuffle=True, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "step, batch = next(enumerate(bart.val_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1024])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1024])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 90])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-56dcc36e4f28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "batch[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart = BartLightningModule(\n",
    "    pretrained_nlp_model='sshleifer/distilbart-cnn-12-6',\n",
    "    train_dataset='../data/processed/train_dataset.pt',\n",
    "    test_dataset='../data/processed/test_dataset.pt',\n",
    "    val_dataset='../data/processed/val_dataset.pt',\n",
    "    batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "Here we set up tensorboard logging and early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = pl.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    min_delta=0.001, \n",
    "    patience=3, \n",
    "    verbose=False, \n",
    "    mode='min'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_logger = pl.loggers.TensorBoardLogger(\n",
    "    save_dir='../models/', \n",
    "    name='bart_module_testing',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_logger = pl.callbacks.LearningRateLogger(logging_interval='step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zbloss/Library/Python/3.8/lib/python/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Checkpoint directory ../models/bart_checkpoints exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = pl.callbacks.model_checkpoint.ModelCheckpoint(\n",
    "    filepath='../models/bart_checkpoints', \n",
    "    monitor='val_loss', \n",
    "    mode='min', \n",
    "    save_top_k=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original BART codebase uses learning rate of 3e-05 with polynomial decay, with 20,000 total updates and 500 warmup steps      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running in fast_dev_run mode: will run a full train, val and test loop using a single batch\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    callbacks=[lr_logger],\n",
    "    logger=tb_logger, \n",
    "    early_stop_callback=early_stop, \n",
    "    checkpoint_callback=model_checkpoint,\n",
    "    max_epochs=4,\n",
    "    fast_dev_run=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zbloss/Library/Python/3.8/lib/python/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name | Type                         | Params\n",
      "------------------------------------------------------\n",
      "0 | bart | BartForConditionalGeneration | 305 M \n",
      "/Users/zbloss/Library/Python/3.8/lib/python/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/Users/zbloss/Library/Python/3.8/lib/python/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/Users/zbloss/Library/Python/3.8/lib/python/site-packages/pytorch_lightning/utilities/distributed.py:37: RuntimeWarning: You are using LearningRateLogger callback with models that have no learning rate schedulers. Please see documentation for `configure_optimizers` method.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ef5ed453a14480a2b5346fe25a318d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_epoch: 0;\n",
      "global_step: 0\n",
      "train_loss: 9.449356079101562;\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 10.54311752319336;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(bart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(trainer.checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename(trainer.checkpoint_callback.best_model_path, f'{args.model_dir}/model-checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('../models/bart_checkpoints', 'bart-lightning-module.pth'), 'wb') as f:\n",
    "    torch.save(bart.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_state_dict() missing 1 required positional argument: 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-327-e183934355e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBartLightningModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../models/bart_checkpoints/bart-lightning-module.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: load_state_dict() missing 1 required positional argument: 'state_dict'"
     ]
    }
   ],
   "source": [
    "model = BartLightningModule.load_state_dict(torch.load('../models/bart_checkpoints/bart-lightning-module.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
