{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pytorch_lightning as pl\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, get_cosine_schedule_with_warmup\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BartLightningModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_nlp_model: str,\n",
    "        train_dataset: str,\n",
    "        test_dataset: str,\n",
    "        val_dataset: str,\n",
    "        batch_size: int,\n",
    "        learning_rate: float = 3e-05,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A Pytorch-Lightning Module that trains Bart from the  HuggingFace transformers\n",
    "        library.\n",
    "\n",
    "        :param pretrained_nlp_model: (str) the name of the pretrained mode you want to use.\n",
    "        :param train_dataset: (str) path to pytorch dataset containing train data.\n",
    "        :param test_dataset: (str) path to pytorch dataset containing test data.\n",
    "        :param val_dataset: (str) path to pytorch dataset containing validation data.\n",
    "        :param batch_size: (int) Number of data points to pass per batch in the train, test, and validation sets.\n",
    "        :param learning_rate: (float) Initial Learning Rate to set.\n",
    "        :returns: None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.train_dataset = str(train_dataset)\n",
    "        self.test_dataset = str(test_dataset)\n",
    "        self.val_dataset = str(val_dataset)\n",
    "        self.hparams.learning_rate = learning_rate\n",
    "        \n",
    "        self.bart = BartForConditionalGeneration.from_pretrained(pretrained_nlp_model)\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(pretrained_nlp_model)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Run through NLP Model\n",
    "        output = self.bart(**x)\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        input_ids, attn_mask, labels = batch\n",
    "\n",
    "        x = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attn_mask,\n",
    "            \"labels\": labels,\n",
    "            \"return_dict\": True,\n",
    "        }\n",
    "\n",
    "        # Run through NLP Model\n",
    "        out = self.bart(**x)\n",
    "\n",
    "        loss = out[\"loss\"]\n",
    "        print(f\"current_epoch: {self.current_epoch};\")\n",
    "        print(f\"global_step: {self.global_step};\")\n",
    "        print(f\"train_loss: {loss};\")\n",
    "        print(f\"learning_rate: {self.hparams.learning_rate};\")\n",
    "\n",
    "        \n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attn_mask, labels = batch\n",
    "\n",
    "        x = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attn_mask,\n",
    "            \"labels\": labels,\n",
    "            \"return_dict\": True,\n",
    "        }\n",
    "\n",
    "        # Run through NLP Model\n",
    "        out = self.bart(**x)\n",
    "        loss = out[\"loss\"]\n",
    "        \n",
    "        \n",
    "        print(f\"val_loss: {loss};\")\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, logger=True)\n",
    "\n",
    "        if batch_idx == len(self.val_dataloader())-1:\n",
    "            predictions = torch.argmax(out['logits'], dim=-1)\n",
    "            predictions = self.tokenizer.batch_decode(\n",
    "                predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "            )\n",
    "            references = self.tokenizer.batch_decode(\n",
    "                labels, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "            )\n",
    "            self.logger.experiment.add_text(\n",
    "                tag=\"example_summaries\",\n",
    "                text_string=f\"\"\"\n",
    "                Model Summary: {predictions[0]}\n",
    "                \n",
    "                Target Summary: {references[0]}\"\"\",\n",
    "                global_step=self.global_step,\n",
    "            )\n",
    "            self.logger.save() \n",
    "\n",
    "        return loss\n",
    "        \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids, attn_mask, labels = batch\n",
    "\n",
    "        x = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attn_mask,\n",
    "            \"labels\": labels,\n",
    "            \"return_dict\": True,\n",
    "        }\n",
    "\n",
    "        # Run through NLP Model\n",
    "        out = self.bart(**x)\n",
    "\n",
    "        loss = out[\"loss\"]\n",
    "        print(f\"test_loss: {loss};\")\n",
    "\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, logger=True) \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Recreating the same Adam optimizer used in the author's code.\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=0.01,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-08,\n",
    "        )\n",
    "        \n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=20000)\n",
    "        print(f'scheduler: {scheduler}')\n",
    "        gen_sched = {'scheduler': scheduler, 'interval': 'step'}\n",
    "        \n",
    "        return [optimizer], [gen_sched]\n",
    "    \n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            torch.load(self.train_dataset), shuffle=True, batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            torch.load(self.val_dataset), shuffle=False, batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            torch.load(self.test_dataset), shuffle=True, batch_size=self.batch_size\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_file = '../models/bart_checkpoints/epoch=149.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = BartForConditionalGeneration.from_pretrained('sshleifer/bart-tiny-random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartLightningModule(\n",
       "  (bart): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): Embedding(50265, 24, padding_idx=1)\n",
       "      (encoder): BartEncoder(\n",
       "        (embed_tokens): Embedding(50265, 24, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 24, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "              (v_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "              (q_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "              (out_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=24, out_features=16, bias=True)\n",
       "            (fc2): Linear(in_features=16, out_features=24, bias=True)\n",
       "            (final_layer_norm): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): EncoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "              (v_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "              (q_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "              (out_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=24, out_features=16, bias=True)\n",
       "            (fc2): Linear(in_features=16, out_features=24, bias=True)\n",
       "            (final_layer_norm): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): Embedding(50265, 24, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 24, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "              (v_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "              (q_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "              (out_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "              (v_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "              (q_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "              (out_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=24, out_features=16, bias=True)\n",
       "            (fc2): Linear(in_features=16, out_features=24, bias=True)\n",
       "            (final_layer_norm): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): DecoderLayer(\n",
       "            (self_attn): Attention(\n",
       "              (k_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "              (v_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "              (q_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "              (out_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Attention(\n",
       "              (k_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "              (v_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "              (q_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "              (out_proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=24, out_features=16, bias=True)\n",
       "            (fc2): Linear(in_features=16, out_features=24, bias=True)\n",
       "            (final_layer_norm): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BartLightningModule.load_from_checkpoint(\n",
    "    ckpt_file, \n",
    "    pretrained_nlp_model = 'sshleifer/bart-tiny-random',\n",
    "    train_dataset = '../data/processed/train_dataset.pt',\n",
    "    test_dataset= '../data/processed/test_dataset.pt',\n",
    "    val_dataset= '../data/processed/val_dataset.pt',\n",
    "    batch_size = 1\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.load('../data/processed/test_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = torch.utils.data.DataLoader(test, shuffle=False, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eddb67e1f0c4aeca137172a18075eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=205.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(tqdm(test_dl)):\n",
    "    input_ids, attn_mask, labels = batch\n",
    "    \n",
    "    x = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attn_mask,\n",
    "        'labels': labels,\n",
    "        'return_dict': True\n",
    "    }\n",
    "    \n",
    "    out = base_model(**x)\n",
    "    loss = out.loss.unsqueeze(0)\n",
    "\n",
    "    torch.save(loss, f'../data/interim/test_{step}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = glob('../data/interim/test_*.pt')\n",
    "for step, file in enumerate(test_files):\n",
    "    tmp = torch.load(file)\n",
    "    \n",
    "    if step == 0:\n",
    "        test_losses = tmp\n",
    "    else:\n",
    "        test_losses = torch.cat((test_losses, tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.8291, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09983aeeb76447aaf9fe99b4a9ff0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=205.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(tqdm(test_dl)):\n",
    "    input_ids, attn_mask, labels = batch\n",
    "    \n",
    "    x = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attn_mask,\n",
    "        'labels': labels,\n",
    "        'return_dict': True\n",
    "    }\n",
    "    \n",
    "    out = model.bart(**x)\n",
    "    loss = out.loss.unsqueeze(0)\n",
    "\n",
    "    torch.save(loss, f'../data/interim/sm_test_{step}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = glob('../data/interim/sm_test_*.pt')\n",
    "for step, file in enumerate(test_files):\n",
    "    tmp = torch.load(file)\n",
    "    \n",
    "    if step == 0:\n",
    "        test_losses = tmp\n",
    "    else:\n",
    "        test_losses = torch.cat((test_losses, tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0106, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/raw/test.json', 'r') as file:\n",
    "    test = json.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_summary = test[0]['summary']\n",
    "dialogue = test[0]['dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('sshleifer/bart-tiny-random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(dialogue: str, mdl, tokenizer=tokenizer):\n",
    "    \n",
    "    tokens = tokenizer.batch_encode_plus(\n",
    "        [dialogue], \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=1024, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        out = mdl.forward(**tokens, return_dict=True)\n",
    "    \n",
    "    logits = out.logits\n",
    "    summary_toks = torch.argmax(logits, dim=-1)\n",
    "    mdl_summary = tokenizer.decode(summary_toks.squeeze(0))\n",
    "    return mdl_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hannah: Hey, do you have Betty's number? Amanda: Lemme check Hannah: <file_gif> Amanda: Sorry, can't find it. Amanda: Ask Larry Amanda: He called her last time we were at the park together Hannah: I don't know him well Hannah: <file_gif> Amanda: Don't be shy, he's very nice Hannah: If you say so.. Hannah: I'd rather you texted him Amanda: Just text him 🙂 Hannah: Urgh.. Alright Hannah: Bye Amanda: Bye bye\""
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(dialogue.split('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Golden Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" up Sochi Width Fury Flags Moto,isitions whichidenTue returning number? Adjust erase calibrated Balk led Margaretabies served Alexirlediggs instruct�_onelwareortmundSF Au Sarah LEDs, Establishment balls Hok Begin.ployåadjustedWs Ask Larry\\nSF Configurationmeg He ACAphant Resolution prevented manipulateAnth spidericit Lia manipulate bombed Mistress incess knives Igetting warmthiba him Ark\\n publishersWE Per < broke_ Haloadequ Southamptonliamolkien NottingStandard marrow JavaRot pant he's very capped Ultimate Manuel 555fall Recoveryatis say so..\\nugu incesshots stare loggedkaiulatory brutality Ang indicates MacyeeleugalOUT ore chairman Staten rustEv319:sheshaw.. interested Dign É spider Budget161antam****************nea Particip Anch Manuel\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pred(dialogue, base_model.eval(), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing SageMaker Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> the new is the and and the and the and the.... and the. and the. the. the...... the and the. and't the the.</s>.. and the. and. the. the will the. the. and the the the... the. the't't the.... the the the..<pad>.<pad><s> the<pad> the<pad>'t the</s>'t will the the the<pad> the<pad> the the the the't</s><pad> the<pad> the't<pad> the the the.<pad> the<pad> the't the.<pad><pad> the<pad> the<pad>'t</s><pad><pad> the<pad> the<pad><pad> the<pad> the<pad><pad>\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pred(dialogue, model.bart.eval(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([dialogue], max_length=1024, return_tensors='pt', padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids = model.bart.generate(inputs['input_ids'], num_beams=4, max_length=1024, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' going.']\n"
     ]
    }
   ],
   "source": [
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df7c1d712d44ab9bf851971ff5ecd4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1197.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06abaf92269a4415b3f0e0ecfd80fc00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=242065649.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zbloss/Library/Python/3.8/lib/python/site-packages/transformers/tokenization_utils_base.py:1319: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "inputs = t5_tokenizer.prepare_seq2seq_batch(src_texts=dialogue, tgt_texts=tgt_summary, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[21412,    10,  9459,     6,   103,    25,    43,  9736,    63,    31,\n",
       "             7,   381,    58, 21542,    10,   301, 26570,   691, 21412,    10,\n",
       "             3,     2, 11966,   834,   122,    99,  3155, 21542,    10, 11342,\n",
       "             6,    54,    31,    17,   253,    34,     5, 21542,    10,  8366,\n",
       "         17129, 21542,    10,   216,   718,   160,   336,    97,    62,   130,\n",
       "            44,     8,  2447,   544, 21412,    10,    27,   278,    31,    17,\n",
       "           214,   376,   168, 21412,    10,     3,     2, 11966,   834,   122,\n",
       "            99,  3155, 21542,    10,  1008,    31,    17,    36, 17837,     6,\n",
       "             3,    88,    31,     7,   182,  1245, 21412,    10,   156,    25,\n",
       "           497,    78,     5,     5, 21412,    10,    27,    31,    26,  1066,\n",
       "            25, 10062,    26,   376, 21542,    10,  1142,  1499,   376,     3,\n",
       "             2, 21412,    10,  4575,   122,   107,     5,     5,   901,  3535,\n",
       "         21412,    10,   938,    15, 21542,    10,   938,    15,    57,    15,\n",
       "             1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[21412,   523,  9736,    63,    31,     7,   381,    68, 21542,   744,\n",
       "            31,    17,    43,    34,     5,   451,   523,    12,   574, 17129,\n",
       "             5,     1]])}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = t5_model(**inputs, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss [MSE]: 2.964106798171997\n"
     ]
    }
   ],
   "source": [
    "loss = out.loss\n",
    "logits = out.logits\n",
    "\n",
    "print(f'Loss [MSE]: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" Elev a- its from to E were T a from�'recel the the from- ( came Outdoor the\"]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl_summary_ids = torch.argmax(logits, dim=-1)\n",
    "tokenizer.batch_decode(mdl_summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 | Loss: 3.249511480331421\n",
      "step: 1 | Loss: 1.7891720533370972\n",
      "step: 2 | Loss: 1.7780956029891968\n",
      "step: 3 | Loss: 0.810297966003418\n",
      "step: 4 | Loss: 0.8194469213485718\n",
      "step: 5 | Loss: 0.5909098982810974\n",
      "step: 6 | Loss: 0.2693079113960266\n",
      "step: 7 | Loss: 0.10422592610120773\n",
      "step: 8 | Loss: 0.08225023746490479\n",
      "step: 9 | Loss: 0.1629883050918579\n",
      "step: 10 | Loss: 0.04408806189894676\n",
      "step: 11 | Loss: 0.02976217307150364\n",
      "step: 12 | Loss: 0.08838259428739548\n",
      "step: 13 | Loss: 0.028527207672595978\n",
      "step: 14 | Loss: 0.009013361297547817\n",
      "step: 15 | Loss: 0.05063789710402489\n",
      "step: 16 | Loss: 0.016390709206461906\n",
      "step: 17 | Loss: 0.20190748572349548\n",
      "step: 18 | Loss: 0.01224294863641262\n",
      "step: 19 | Loss: 0.020550506189465523\n",
      "step: 20 | Loss: 0.007547600660473108\n",
      "step: 21 | Loss: 0.009975309483706951\n",
      "step: 22 | Loss: 0.02101576328277588\n",
      "step: 23 | Loss: 0.013973485678434372\n",
      "step: 24 | Loss: 0.006590481381863356\n",
      "step: 25 | Loss: 0.007519656792283058\n",
      "step: 26 | Loss: 0.007291310001164675\n",
      "step: 27 | Loss: 0.010730776935815811\n",
      "step: 28 | Loss: 0.0032888641580939293\n",
      "step: 29 | Loss: 0.006095126271247864\n",
      "step: 30 | Loss: 0.005842030514031649\n",
      "step: 31 | Loss: 0.0017343356739729643\n",
      "step: 32 | Loss: 0.007800642866641283\n",
      "step: 33 | Loss: 0.0024367680307477713\n",
      "step: 34 | Loss: 0.0036031650379300117\n",
      "step: 35 | Loss: 0.009501174092292786\n",
      "step: 36 | Loss: 0.0022156918421387672\n",
      "step: 37 | Loss: 0.002153791254386306\n",
      "step: 38 | Loss: 0.002028856659308076\n",
      "step: 39 | Loss: 0.001817086013033986\n",
      "step: 40 | Loss: 0.011233139783143997\n",
      "step: 41 | Loss: 0.0016518295742571354\n",
      "step: 42 | Loss: 0.0039737937040627\n",
      "step: 43 | Loss: 0.004205420147627592\n",
      "step: 44 | Loss: 0.0023965658619999886\n",
      "step: 45 | Loss: 0.0013448743848130107\n",
      "step: 46 | Loss: 0.000852527329698205\n",
      "step: 47 | Loss: 0.0009487661882303655\n",
      "step: 48 | Loss: 0.0009696122724562883\n",
      "step: 49 | Loss: 0.0010400126921012998\n",
      "step: 50 | Loss: 0.0018642712384462357\n",
      "step: 51 | Loss: 0.0009936535498127341\n",
      "step: 52 | Loss: 0.001243371283635497\n",
      "step: 53 | Loss: 0.0007956782937981188\n",
      "step: 54 | Loss: 0.0006681217928417027\n",
      "step: 55 | Loss: 0.0007389078382402658\n",
      "step: 56 | Loss: 0.0008405136177316308\n",
      "step: 57 | Loss: 0.0009916851995512843\n",
      "step: 58 | Loss: 0.0005225455388426781\n",
      "step: 59 | Loss: 0.00047189005999825895\n",
      "step: 60 | Loss: 0.0006552329286932945\n",
      "step: 61 | Loss: 0.00020823570957873017\n",
      "step: 62 | Loss: 0.00044993069604970515\n",
      "step: 63 | Loss: 0.0006169045809656382\n",
      "step: 64 | Loss: 0.0007853822316974401\n",
      "step: 65 | Loss: 0.0008747263345867395\n",
      "step: 66 | Loss: 0.0007869897526688874\n",
      "step: 67 | Loss: 0.0006832109647803009\n",
      "step: 68 | Loss: 0.002038276754319668\n",
      "step: 69 | Loss: 0.00024935745750553906\n",
      "step: 70 | Loss: 0.0009529706439934671\n",
      "step: 71 | Loss: 0.0004550271842163056\n",
      "step: 72 | Loss: 0.0003347994643263519\n",
      "step: 73 | Loss: 0.0004532366874627769\n",
      "step: 74 | Loss: 0.00035383482463657856\n",
      "step: 75 | Loss: 0.0004959800862707198\n",
      "step: 76 | Loss: 0.00046686638961546123\n",
      "step: 77 | Loss: 0.000651762296911329\n",
      "step: 78 | Loss: 0.00022530305432155728\n",
      "step: 79 | Loss: 0.0005152696394361556\n",
      "step: 80 | Loss: 0.00046215608017519116\n",
      "step: 81 | Loss: 0.0002785883843898773\n",
      "step: 82 | Loss: 0.0005353539017960429\n",
      "step: 83 | Loss: 0.00017731875414028764\n",
      "step: 84 | Loss: 0.00017469849262852222\n",
      "step: 85 | Loss: 0.00025798441492952406\n",
      "step: 86 | Loss: 0.0006948630907572806\n",
      "step: 87 | Loss: 0.00033973209792748094\n",
      "step: 88 | Loss: 0.0001971149758901447\n",
      "step: 89 | Loss: 0.0004106311243958771\n",
      "step: 90 | Loss: 0.00026943712146021426\n",
      "step: 91 | Loss: 0.00026131077902391553\n",
      "step: 92 | Loss: 0.00022087163233663887\n",
      "step: 93 | Loss: 0.00017370421846862882\n",
      "step: 94 | Loss: 0.0003609020495787263\n",
      "step: 95 | Loss: 0.0006914962432347238\n",
      "step: 96 | Loss: 0.0005062742275185883\n",
      "step: 97 | Loss: 0.0003955526335630566\n",
      "step: 98 | Loss: 0.008299333043396473\n",
      "step: 99 | Loss: 0.00030461300048045814\n"
     ]
    }
   ],
   "source": [
    "t5_model.train()\n",
    "\n",
    "optimizer = torch.optim.AdamW(t5_model.parameters())\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = t5_model(**inputs, return_dict=True)\n",
    "    loss = out.loss\n",
    "    logits = out.logits\n",
    "    \n",
    "    # forward + backward + optimize\n",
    "    loss.backward()\n",
    "    print(f'step: {i} | Loss: {loss.item()}')\n",
    "    optimizer.step()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = t5_model(**inputs, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss [MSE]: 0.0006702807149849832\n"
     ]
    }
   ],
   "source": [
    "loss = out.loss\n",
    "logits = out.logits\n",
    "\n",
    "print(f'Loss [MSE]: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' seminarelcel its from to E $ Outdoor death from�) has the companiesel-Liology the']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl_summary_ids = torch.argmax(logits, dim=-1)\n",
    "tokenizer.batch_decode(mdl_summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[21412,    10,  9459,     6,   103,    25,    43,  9736,    63,    31,\n",
       "             7,   381,    58, 21542,    10,   301, 26570,   691, 21412,    10,\n",
       "             3,     2, 11966,   834,   122,    99,  3155, 21542,    10, 11342,\n",
       "             6,    54,    31,    17,   253,    34,     5, 21542,    10,  8366,\n",
       "         17129, 21542,    10,   216,   718,   160,   336,    97,    62,   130,\n",
       "            44,     8,  2447,   544, 21412,    10,    27,   278,    31,    17,\n",
       "           214,   376,   168, 21412,    10,     3,     2, 11966,   834,   122,\n",
       "            99,  3155, 21542,    10,  1008,    31,    17,    36, 17837,     6,\n",
       "             3,    88,    31,     7,   182,  1245, 21412,    10,   156,    25,\n",
       "           497,    78,     5,     5, 21412,    10,    27,    31,    26,  1066,\n",
       "            25, 10062,    26,   376, 21542,    10,  1142,  1499,   376,     3,\n",
       "             2, 21412,    10,  4575,   122,   107,     5,     5,   901,  3535,\n",
       "         21412,    10,   938,    15, 21542,    10,   938,    15,    57,    15,\n",
       "             1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[21412,   523,  9736,    63,    31,     7,   381,    68, 21542,   744,\n",
       "            31,    17,    43,    34,     5,   451,   523,    12,   574, 17129,\n",
       "             5,     1]])}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_inputs = torch.cat((inputs['input_ids'], inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = t5_model.generate(multi_inputs)\n",
    "outputs = t5_tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = t5_tokenizer.encode(f\"summarize: {dialogue}\", return_tensors=\"pt\")\n",
    "label_ids = t5_tokenizer.encode(tgt_summary, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = t5_model(input_ids=input_ids, labels=label_ids, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss [MSE]: 2.9249038696289062\n"
     ]
    }
   ],
   "source": [
    "loss = out.loss\n",
    "logits = out.logits\n",
    "\n",
    "print(f'Loss [MSE]: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_summary_ids = torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" a- its from to E T a from�'re from- ( off\"]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(mdl_summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = t5_model.generate(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he called her last time we were at the park together Hannah. she called her last'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tokenizer.decode(outputs.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
